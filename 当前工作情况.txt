2018/2/2  舆情发现功能基本实现，但分词阶段处理速度慢、内存占用大。还需对聚类结果进行重新排序，以簇节点数由大到小排序，便于观察聚类结果。
2018/2/27  推测聚类处理时，由于每次执行single-pass需要比对之前所有内容，导致数据的处理量飞速上升，加上字典本身过于庞大形成了极高纬度的向量，最终使得运算变得非常缓慢。尝试减小字典大小，仅制作针对某一具体领域的字典并收集该领域的热点。
2018/2/28  已寻得python内置的时间日期计算函数，下一步将时间对舆情的影响也纳入计算阈值的考量。
2018/3/2  已将时间因素纳入相关性计算，目前用笔记本运行2000条数据耗时60分钟左右，下一步尝试使用服务器进行运行。注：单纯将numpy生成的数组调整为float16时，虽节约了内存但会大幅降低运算速度，目测相差十倍，节约内存需另想他法。
2018/3/9  目前依旧是利用了one-hot与TF-IDF相结合构造的VSM模型。过大的向量维度会导致维度灾难现象，最终占用大量的内存并使得计算变得极其缓慢。尝试学习doc2vec技术来代替传统的VSM模型。
2018/3/12  已在网上寻得可用的预训练的word2vec词向量，正在进一步调试其可用性与准确性。
2018/3/13  词嵌入确实有效地解决了维度灾难，计算速度得到显著提升。但是由于其特征向量不具备相加性，因此难以作为可逐步改进的质心，也难以简单地用多个词来代表句子的特征，接下来尝试用一定条件来删选出单个词来代替句子的特征。
2018/3/16  先前使用word2vec时，词语与向量的匹配存在错误，现已修复正常。经过一系列数据测试发现，将词向量以tfidf值加权累加平均后获得的句向量较能代表短文本本身的含义，可通过调节关键词个数与阈值来获得较为合理直观的聚类结果。（关键词的多少与句子的长度相关，阈值的大小与噪声的大小相关）
2018/3/19  为了代码的健壮性和高效性考虑，现将无有效内容的结点与时间提取错误的结点直接排除。为了进一步加快single-pass的聚类，现将从xml文件读取的内容分成多块进行处理，结果与原来基本一致，方案可行。（分块前10000条数据耗时60分钟左右，分块后耗时20分钟左右）
2018/3/20  现将xml读取的内容以每1000个一组的形式进行single-pass运算，最终全部计算完成（超过16万条数据）耗时不到3小时，判断程序拥有可执行性。运算结果利用pickle保存。删除部分可能对聚类有负面作用的词汇。
2018/3/21  为了避免常用词在短文本中对聚类造成的干扰，设立了stopwords.txt来针对处理。经过大量实验测试后，不宜再调整关键词个数与阈值大小，但可以调整时间距离的权值来更好地匹配出需要的类。
2018/3/22  目前正在等待新爬取的数据做测试，需重新制定stopword.txt。开始着手撰写论文，同时进行情感分析模块的研究。
2018/4/3  还在撰写论文中，稍许对single-pass的控制参数和形式进行修改，使之更为直观。
2018/4/10  修改部分Single-Pass参数使之通用性更好，正在尝试使用新的balltree算法来加速Single-pass聚类，但是目前难以寻得中文说明。
2018/4/12  balltree暂时无法配合提升Single-Pass性能，现在正在添加聚类结果显示关键词的功能。
2018/4/13  对聚类的输出结果稍作调整，使其更为直观。